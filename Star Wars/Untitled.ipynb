{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SW_EpisodeIV.txt', 'SW_EpisodeV.txt', 'SW_EpisodeVI.txt', 'wordcloud_masks']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras as K\n",
    "import random\n",
    "import sqlite3\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, concatenate, Embedding\n",
    "from tensorflow.keras.layers import Flatten, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM, CuDNNGRU, CuDNNLSTM\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "print(os.listdir(\"datasets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_SW_Scripts = \"\"\n",
    "\n",
    "def TextToString(txt):\n",
    "    with open (txt, \"r\") as file:\n",
    "        data=file.readlines()\n",
    "        script = \"\"\n",
    "        for x in data[1:-1]:\n",
    "            x = x.lower().replace('\"','').replace(\"\\n\",\" \\n \").split(' ')\n",
    "            x[1] += \":\"\n",
    "            script += \" \".join(x[1:-1]).replace(\"\\n\",\" \\n \")\n",
    "        return script\n",
    "    \n",
    "All_SW_Scripts += TextToString(\"datasets/SW_EpisodeIV.txt\")\n",
    "All_SW_Scripts += TextToString(\"datasets/SW_EpisodeV.txt\")\n",
    "All_SW_Scripts += TextToString(\"datasets/SW_EpisodeVI.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threepio: did you hear that?  they've shut down the main reactor.  we'll be destroyed for sure.  this is madness!  \n",
      " threepio: we're doomed!  \n",
      " threepio: there'll be no escape for the princess this time.  \n",
      " threepio: what's that?  \n",
      " threepio: i should have known better than to trust the logic of a half-sized thermocapsulary dehousing assister...  \n",
      " luke: hurry up!  come with me!  what are you waiting for?!  get in gear!  \n",
      " threepio: artoo! artoo-detoo, where are you?  \n",
      " threepio: at last!  where have you been?  \n",
      " threepio: they're heading in this direction. what are we going to do?  we'll be sent to the spice mines of kessel or smashed into who knows what!  \n",
      " threepio: wait a minute, where are you going?  \n",
      " imperial: officer the death star plans are not in the main computer.  \n",
      " vader: where are those transmissions you intercepted?  \n",
      " rebel: officer we intercepted no transmissions. aaah...  this is a consular ship. were on a diplomatic mission.  \n",
      " vader: if this is a consular ship... wh\n"
     ]
    }
   ],
   "source": [
    "print(All_SW_Scripts[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"All_SW_Scripts.txt\", \"w\")\n",
    "text_file.write(All_SW_Scripts)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '6', '7', '8', ':', ';', '?', '\\\\', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "166969\n"
     ]
    }
   ],
   "source": [
    "Text_Data = All_SW_Scripts\n",
    "\n",
    "charindex = list(set(Text_Data))\n",
    "charindex.sort() \n",
    "print(charindex)\n",
    "\n",
    "np.save(\"charindex.npy\", charindex)\n",
    "\n",
    "print(len(Text_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CHARS_SIZE = len(charindex)\n",
    "SEQUENCE_LENGTH = 100\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for i in range(0, len(Text_Data)-SEQUENCE_LENGTH, 1 ): \n",
    "    X = Text_Data[i:i + SEQUENCE_LENGTH]\n",
    "    Y = Text_Data[i + SEQUENCE_LENGTH]\n",
    "    X_train.append([charindex.index(x) for x in X])\n",
    "    Y_train.append(charindex.index(Y))\n",
    "\n",
    "X_train = np.reshape(X_train, (len(X_train), SEQUENCE_LENGTH))\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166869, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166869, 47)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Wasin\\Programming\\anaconda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From D:\\Wasin\\Programming\\anaconda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 100)          4700      \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)       (None, 100, 512)          1257472   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 100, 512)          2101248   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 512)               2101248   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 47)                6063      \n",
      "=================================================================\n",
      "Total params: 5,634,955\n",
      "Trainable params: 5,630,255\n",
      "Non-trainable params: 4,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    inp = Input(shape=(SEQUENCE_LENGTH, ))\n",
    "    x = Embedding(CHARS_SIZE, 100, trainable=False)(inp)\n",
    "    x = CuDNNLSTM(512, return_sequences=True,)(x)\n",
    "    x = CuDNNLSTM(512, return_sequences=True,)(x)\n",
    "    x = CuDNNLSTM(512,)(x)\n",
    "    x = Dense(256, activation=\"elu\")(x)\n",
    "    x = Dense(128, activation=\"elu\")(x)\n",
    "    outp = Dense(CHARS_SIZE, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=['accuracy'],\n",
    "                 )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSample(Callback):\n",
    "\n",
    "    def __init__(self):\n",
    "       super(Callback, self).__init__() \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pattern = X_train[700]\n",
    "        outp = []\n",
    "        seed = [charindex[x] for x in pattern]\n",
    "        sample = 'TextSample:' +''.join(seed)+'|'\n",
    "        for t in range(100):\n",
    "          x = np.reshape(pattern, (1, len(pattern)))\n",
    "          pred = self.model.predict(x)\n",
    "          result = np.argmax(pred)\n",
    "          outp.append(result)\n",
    "          pattern = np.append(pattern,result)\n",
    "          pattern = pattern[1:len(pattern)]\n",
    "        outp = [charindex[x] for x in outp]\n",
    "        outp = ''.join(outp)\n",
    "        sample += outp\n",
    "        print(sample)\n",
    "\n",
    "textsample = TextSample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"model_checkpoint.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                             monitor='loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"loss\",\n",
    "                      mode=\"min\",\n",
    "                      patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 166869 samples\n",
      "Epoch 1/40\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.98613, saving model to model_checkpoint.hdf5\n",
      "166869/166869 - 230s - loss: 1.9861 - acc: 0.4306\n",
      "Epoch 2/40\n",
      "\n",
      "Epoch 00002: loss improved from 1.98613 to 1.38387, saving model to model_checkpoint.hdf5\n",
      "166869/166869 - 235s - loss: 1.3839 - acc: 0.5846\n",
      "Epoch 3/40\n",
      "\n",
      "Epoch 00003: loss improved from 1.38387 to 1.18989, saving model to model_checkpoint.hdf5\n",
      "166869/166869 - 236s - loss: 1.1899 - acc: 0.6351\n",
      "Epoch 4/40\n",
      "\n",
      "Epoch 00004: loss improved from 1.18989 to 1.07270, saving model to model_checkpoint.hdf5\n",
      "166869/166869 - 235s - loss: 1.0727 - acc: 0.6656\n",
      "Epoch 5/40\n",
      "\n",
      "Epoch 00005: loss improved from 1.07270 to 0.98221, saving model to model_checkpoint.hdf5\n",
      "166869/166869 - 235s - loss: 0.9822 - acc: 0.6903\n",
      "Epoch 6/40\n",
      "\n",
      "Epoch 00006: loss improved from 0.98221 to 0.89610, saving model to model_checkpoint.hdf5\n",
      "166869/166869 - 238s - loss: 0.8961 - acc: 0.7122\n",
      "Epoch 7/40\n",
      "\n",
      "Epoch 00007: loss improved from 0.89610 to 0.81747, saving model to model_checkpoint.hdf5\n",
      "166869/166869 - 242s - loss: 0.8175 - acc: 0.7348\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-118fedccc5bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m           callbacks = model_callbacks)\n\u001b[0m",
      "\u001b[1;32mD:\\Wasin\\Programming\\anaconda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Wasin\\Programming\\anaconda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Wasin\\Programming\\anaconda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Wasin\\Programming\\anaconda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mD:\\Wasin\\Programming\\anaconda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_callbacks = [checkpoint, early]\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=50,\n",
    "          epochs=40,\n",
    "          verbose=2,\n",
    "          callbacks = model_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(filepath)\n",
    "model.save_weights(\"full_train_weights.hdf5\")\n",
    "model.save(\"full_train_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
